# ğŸ™ï¸ Speech Analysis System

An end-to-end **AI-powered speech analysis system** that evaluates how a person speaks â€” not *what* they say. The system provides clear, actionable feedback on speech quality using a combination of **deep learning models**, **automatic speech recognition (ASR)**, and **audio signal processing**.

This project is designed as a **real-world, product-style pipeline**, focusing on explainable metrics, clean UX, and scalability for future features.

---

## ğŸš€ Features

* ğŸ­ **Emotion Classification** â€“ Detects speaking emotion (e.g., calm, nervous, confident)
* ğŸ§‘ **Gender Classification** â€“ Predicts speaker gender (used as contextual information)
* ğŸ‚ **Age Group Estimation** â€“ Predicts speaker age range (not exact age)
* ğŸŒ **Accent Classification** â€“ Identifies speaker accent and accent strength
* ğŸ“ **Automatic Transcription (ASR)** â€“ Converts speech to text using Whisper
* ğŸ“Š **Speech Quality Metrics**:

  * Confidence Score
  * Clarity Score
  * Fluency Score
  * Accent Strength Score
* ğŸ“ˆ **Progress Tracking** â€“ Shows percentage improvement from previous sessions

---

## ğŸ§  System Architecture (High Level)

```
Audio Input
 â”œâ”€â”€ Emotion Model (DL)
 â”œâ”€â”€ Age Model (DL)
 â”œâ”€â”€ Gender Model (DL)
 â”œâ”€â”€ Accent Model (DL)
 â””â”€â”€ ASR (Whisper)
       â”œâ”€â”€ Transcript
       â”œâ”€â”€ Words Per Minute (WPM)
       â”œâ”€â”€ Pause Detection
       â”œâ”€â”€ Filler Word Detection
       â””â”€â”€ Speech Continuity

All outputs â†’ Metric Aggregation â†’ Dashboard Scores
```

---

## ğŸ§¾ Automatic Speech Recognition (ASR)

The system uses **OpenAI Whisper**, a transformer-based ASR model, to generate transcripts from audio.

The transcript enables:

* Words Per Minute (WPM)
* Fluency analysis
* Pause detection using timestamps
* Vocabulary and filler word analysis

Whisper is used as a **pretrained model** and is not fine-tuned in this project.

---

## ğŸ“Š Speech Metrics Explained

### ğŸ”¹ 1. Confidence Score (0â€“100)

Represents how confident the speaker sounds.

Computed using:

* Emotion model output (confidence-related emotions)
* Loudness / energy of speech
* Pause frequency

**Interpretation:**

* High score â†’ stable, energetic, confident speech
* Low score â†’ hesitant or nervous delivery

---

### ğŸ”¹ 2. Clarity Score (0â€“100)

Represents how easy the speech is to understand.

This version **does not use a pronunciation model**. Instead, it relies on explainable acoustic and linguistic signals.

Computed using:

* Signal-to-Noise Ratio (background noise)
* Speech vs silence ratio
* Spectral articulation sharpness
* Pause density (ASR difficulty proxy)

**Why no pronunciation model?**

* Keeps v1 lightweight and stable
* Avoids need for phoneme-level datasets
* Easy to upgrade in future versions

---

### ğŸ”¹ 3. Fluency Score (0â€“100)

Measures how smoothly the speaker talks.

Computed using:

* Words Per Minute (WPM)
* Average pause duration
* Pause frequency

**Interpretation:**

* High score â†’ smooth, natural speech flow
* Low score â†’ frequent stops or uneven pacing

---

### ğŸ”¹ 4. Accent Strength Score (0â€“100)

Measures how strong a speakerâ€™s accent is relative to a neutral reference.

Computed using:

* Accent classification confidence

**Important:**

* This is **not a quality judgment**
* It is used to provide pronunciation-related feedback

---

## ğŸ“ˆ Progress Tracking

For each session, scores are compared with the **previous analysis period**.

### Percentage Change Formula:

```
% change = ((current_score - previous_score) / previous_score) Ã— 100
```

This allows users to:

* Track improvement over time
* See trends instead of isolated scores

---

## ğŸ—‚ï¸ Data Storage Strategy

Only **lightweight, user-relevant data** is stored:

* Overall score
* Sub-scores (confidence, clarity, fluency, accent)
* Timestamp
* Improvement percentage

The following are **not stored by default**:

* Raw audio
* Model embeddings
* Raw transcripts (optional, privacy-first)

---

## ğŸŒ Frontend Dashboard

The frontend presents results using:

* Metric cards (score + % change)
* Color-coded indicators
* Short improvement tips
* Trend graphs over time

The UI is designed to be:

* Simple
* Explainable
* Non-technical

---

## ğŸ› ï¸ Tech Stack

### Machine Learning

* PyTorch
* Hugging Face Transformers
* OpenAI Whisper (ASR)
* Librosa (audio signal processing)

### Backend

* Python
* FastAPI (for Model interaction)
* Node.js, Express.js (for main backend tasks)

### Frontend

* React
* Tailwind CSS
* shadcn UI

### Development

* Google Colab (training & experiments)
* vs code (for frontend and backend integration and development)

---

## ğŸ§­ Future Improvements (Roadmap)

* ğŸ” Multitask model with shared audio encoder
* ğŸ—£ï¸ Pronunciation scoring using phoneme alignment
* ğŸ¶ Singing analysis module
* ğŸ§ Song matching & similarity scoring
* ğŸ‘¤ User profiles with long-term analytics

---

## ğŸ¯ Project Goal

This project focuses on **building a practical, end-to-end speech analysis product**, not just individual ML models. Emphasis is placed on:

* Explainability
* Clean data flow
* User-focused feedback
* Real-world engineering decisions

---

## ğŸ“Œ Disclaimer

This system is intended for **educational and self-improvement purposes only**. Predictions related to age, gender, or accent are probabilistic and should not be used for sensitive or decision-critical applications.

---

## ğŸ™Œ Author

Built with â¤ï¸ as a full-stack + ML project to demonstrate practical AI system design.
